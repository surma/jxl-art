<!doctype html>
<title>JXL Art</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="manifest" href="<%= emitEjs('src/manifest.json.ejs') %>" />
<link rel="stylesheet" href="<%= emitAsset('src/styles.css') %>" />
<meta charset="UTF-8" />

<body class="text">
  <p>
    <strong
      >JXL Art is the practice of using
      <a href="https://jpeg.org/jpegxl/">JPEG XL</a>’s prediction tree to
      generate art</strong
    >. If you have questions, you can join the <code>#jxl-art</code> channel on
    the <a href="https://discord.gg/DqkQgDRTFu">JPEG XL Discord</a>.
  </p>

  <p>
    The oversimplified summary is that JPEG XL has a modular mode that divides
    the image it encodes into squares called groups, up to 1024x1024 each. JPEG
    XL uses a prediction tree to make a prediction what the value of each pixel
    is in such a square, based on neighboring pixels and their gradients. As a
    result only the <em>difference</em> (or error) between the
    <em>actual</em> image and the prediction needs to be encoded. The better the
    predictions, the smaller the error, the more compressible the data, the
    smaller the file size. Profit.
  </p>

  <p>
    In the context of JXL art, however, the error is always assumed to be zero,
    which makes the image consist of <em>only</em> the prediction tree. That
    means the predictions effectively generate the image. The process of
    creating JXL art is writing that prediction tree. A prediction tree is a
    tree of if-else statements, branching on different properties and selecting
    a predictor for each leaf of the tree. The flexibility of these prediction
    trees is intentionally limited because they need to be small and execution
    needs to be fast, as it is part of the image decoding process. The
    prediction tree is run for every channel (red, green and blue) and for every
    pixel. Some predictors incorporate the values of the current pixel’s
    neighbors, specifically to the left, top-left, top and top-right, which
    dictates in which order the pixels have to be predicted: Row-by-row,
    top-to-bottom, left-to-right — just like Westerners read text.
  </p>

  <p>
    A program starts with an optional header that specifies image properties and
    transformations to apply. The default header looks like this (everything is
    optional):
  </p>
  <ul>
    <li><code>Width 1024</code>: Width of the image (frame)</li>
    <li><code>Height 1024</code>: Height of the image (frame)</li>
    <li>
      <code>RCT 0</code>: Reversible Color Transform.
      <ul>
        <li>0 is no transform, i.e. RGB.</li>
        <li>6 is YCoCg.</li>
        <li>Higher numbers can be used (up to 42 excluded).</li>
      </ul>
    </li>
    <li>
      <code>Orientation 0</code>: Image rotation/flip as specified by EXIF.
      <ol start="0">
        <li>= 0 degrees</li>
        <li>= 0 degrees, mirrored</li>
        <li>= 180 degrees</li>
        <li>= 180 degrees, mirrored</li>
        <li>= 90 degrees</li>
        <li>= 90 degrees, mirrored</li>
        <li>= 270 degrees</li>
        <li>= 270 degrees, mirrored</li>
      </ol>
    </li>

    <li>
      <code>XYB</code>: Use XYB color space instead of RGB (not enabled by
      default).
    </li>
    <li>
      <code>CbYCr</code>: Use YCbCr color space. Channel 0 becomes Cb, channel 1
      is Y, channel 2 is Cr (not enabled by default).
    </li>
    <li>
      <code>GroupShift 3</code>: Set the group size to
      <code>128 << GroupShift</code>. Values 0-3 are valid.
    </li>
    <li>
      <code>Bitdepth 8</code>: Self-explanatory. Other bit depths can be used,
      from 1 to 31.
    </li>
    <li>
      <code>FloatExpBits 3</code>: Numbers are interpreted as IEEE floats with
      this many bits for the exponent (not enabled by default).
    </li>
    <li>
      <code>Alpha</code>: Add a fourth channel (c == 3) for Alpha (not enabled
      by default).
    </li>
    <li>
      <code>Squeeze</code>: Apply the Squeeze transform (not enabled by
      default). Weird things will happen and the image gets many channels; keep
      predictor values low to avoid blocky images.
    </li>
    <li>
      <code>FramePos 0 0</code>: The frame position is set to this (x0, y0)
      position. The image canvas size also gets adjusted so the bottom right
      corner of the frame remains in the bottom right corner of the image. This
      is mostly useful with negative values, e.g.
      <code>FramePos -100 -200</code>, which has the effect of hiding the first
      100 columns and the first 200 rows.
    </li>
    <li>
      <code>NotLast</code>: This is not the last frame/layer (not enabled by
      default). This flag can be used to do multi-layer images. After encoding
      this layer, another layer will be encoded, which gets alpha-blended over
      the first layer and which can itself also have the NotLast option (there
      is no limit on the number of layers you can create this way). Every layer
      gets its own tree, so when this flag is used, you should specify not just
      one tree, but (at least) two. You can change the RCT between layers (they
      are local transforms), but not the Bitdepth, XYB, Orientation or presence
      of Alpha, since those are file/global properties/transforms. (without
      Alpha it is not very useful at the moment to do layering, since only
      alpha-blending can currently be done, though this will change when we add
      support for other blend modes and/or animation)
    </li>
    <li>
      <code>Spline [4 * 32 coefficients] x0 y0 x1 y1 ... xn yn EndSpline</code>:
      Draws a spline that goes through the points (x0,y0), (x1,y1), ..., (xn,yn)
      and that has a color and thickness given by 4 times 32 numbers. These 32
      numbers are 1D-DCT32 coefficients and floating point numbers, where the
      first number is the DC (i.e. the average value) and the next numbers
      correspond to increasing frequencies. The first series of 32 numbers
      corresponds to the color of the first channel (e.g. Red), where 1.0 is the
      maximum value. The second series corresponds to the second channel (e.g.
      Green), the third to the third channel (e.g. Blue). The final series of 32
      numbers defines the 'thickness' in pixels (note that it's more like a blur
      radius than a thickness of a solid line). The spline gets 'added' to the
      frame, so negative numbers (both in colors and in thickness) correspond to
      darkening while positive numbers correspond to brightening.
    </li>
  </ul>

  <p>
    It is then followed by a tree description, which starts with a decision node
    — an if-else-like statement. Technically you can also just give a single
    predictor, but that’s rarely interesting. A decision node looks like this:
  </p>

  <pre>
if [property] > [value:int]
   (THEN BRANCH)
   (ELSE BRANCH)
</pre
  >

  <p>
    Both the <code>THEN</code> branch and the <code>ELSE</code> branch can
    either be another decision node or a leaf node.
  </p>

  <p>The following properties can be used in a decision node:</p>

  <ul>
    <li>
      <code>c</code>: the channel number, where 0=R, 1=G, 2=B, 3=A (if Alpha was
      enabled in the header)
    </li>
    <li>
      <code>g</code>: the group number (useful in case the image is larger than
      one group). Modular group numbers usually start with 21.
    </li>
    <li><code>x</code>, <code>y</code>: coordinates</li>
    <li><code>N</code>: value of pixel above (north)</li>
    <li><code>W</code>: value of pixel to the left (west)</li>
    <li><code>|N|</code>: absolute value of pixel above (north)</li>
    <li><code>|W|</code>: absolute value of pixel to the left (west)</li>
    <li>
      <code>W-WW-NW+NWW</code>: basically the error of the gradient predictor
      for the pixel on the left
    </li>
    <li><code>W+N-NW</code>: value of gradient predictor (before clamping)</li>
    <li>
      <code>W-NW</code>: left minus topleft, i.e. error of the
      <code>N</code> predictor for the pixel on the left
    </li>
    <li>
      <code>NW-N</code>: topleft minus top, i.e. error of
      <code>W</code> predictor for the pixel above
    </li>
    <li>
      <code>N-NE</code>: top minus topright, i.e. error of
      <code>W</code> predictor for pixel on top right
    </li>
    <li>
      <code>N-NN</code>: top minus toptop, i.e. error of
      <code>N</code> predictor for pixel above
    </li>
    <li>
      <code>W-WW</code>: left minus leftleft, i.e. error of
      <code>W</code> predictor for the pixel on the left
    </li>
    <li><code>WGH</code>: signed max-absval-error of the weighted predictor</li>
    <li>
      <code>Prev</code>: the pixel value in this position in the
      <i>previous</i> channel
    </li>
    <li>
      <code>PPrev</code>: the pixel value in this position in the channel before
      the previous channel
    </li>
    <li>
      <code>PrevErr</code>: the difference between pixel value and the
      <code>Gradient</code>-predicted value in this position in the
      <i>previous</i> channel
    </li>
    <li>
      <code>PPrevErr</code>: like <code>PrevErr</code>, but for the channel
      before that
    </li>
    <li>
      <code>PrevAbs, PPrevAbs, PrevAbsErr, PPrevAbsErr</code>: same as the
      above, but the absolute value (not the signed value)
    </li>
  </ul>

  <p>Leaf nodes are of the following form:</p>

  <pre>
  - [predictor] +-[offset:int]
</pre
  >

  <p>The following predictors are supported in leaf nodes:</p>

  <ul>
    <li>
      <code>Set</code>: always predicts zero, so effectively sets the pixel
      value to [offset]
    </li>
    <li><code>W</code>: value of pixel on the left</li>
    <li><code>N</code>: value of pixel above</li>
    <li><code>NW</code>: value of topleft pixel</li>
    <li><code>NE</code>: value of topright pixel</li>
    <li>
      <code>WW</code>: value of pixel to the left of the pixel on the left
    </li>
    <li><code>Select</code>: predictor from lossless WebP</li>
    <li>
      <code>Gradient</code>: <code>W+N-NW</code>, clamped to
      <code>min(W,N)..max(W,N)</code>
    </li>
    <li>
      <code>Weighted</code>: weighted sum of 4 self-correcting subpredictors
      based on their past performance (warning: not clamped so can get out of
      range)
    </li>
    <li>
      <code>AvgW+N</code
      >,<code>AvgW+NW</code>,<code>AvgN+NW</code>,<code>AvgN+NE</code>: average
      of two pixel values
    </li>
    <li>
      <code>AvgAll</code>: weighted sum of various pixels:
      <code
        >(6 * top - 2 * toptop + 7 * left + 1 * leftleft + 1 * toprightright + 3
        * topright + 8) / 16</code
      >
    </li>
  </ul>

  <p>Edge cases:</p>

  <ul>
    <li>
      If <code>x=y=0</code>, <code>W</code> is set to 0. Otherwise, if
      <code>x=0</code>, <code>W</code> is set to <code>N</code>.
    </li>
    <li>If <code>y=0</code>, <code>N</code> is set to <code>W</code>.</li>
    <li>
      If <code>x=0</code> or <code>y=0</code>, <code>NW</code> is set to
      <code>W</code>.
    </li>
    <li>
      Similarly, <code>NE</code> and <code>NN</code> fall back to
      <code>N</code> and <code>WW</code> falls back to <code>W</code>.
    </li>
  </ul>
  <footer>
    Commit <%= meta.currentCommit %>
    <button class="button" id="nuke">Nuke cache ☣️</button>
  </footer>
  <script src="<%= emitChunk('src/wtf.js'); %>"></script>
</body>
